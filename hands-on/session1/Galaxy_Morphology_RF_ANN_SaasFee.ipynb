{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Galaxy_Morphology_RF_ANN_SaasFee.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/Saas-Fee/blob/main/hands-on/session1/Galaxy_Morphology_RF_ANN_SaasFee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jatIhJ9KJQV"
      },
      "source": [
        "#Galaxy Morphology with feauture based ML: Random Forests, ANNs\n",
        "\n",
        "The goal of these tutorial series is to illustrate a very basic supervised binary classification with different manual features based ML algorithms. The goal is to setup a ML algorithm to determine the visual morphological type of nearby galaxies from the Sloan Digital Sky Survey. The first deep learning papers in Astronomy addressed this problem at low and high redshift (Dielemann+15, Huertas-Company+15).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1TaiRB1wxui4AKnhuF4iH4LJkmrlb-D6d)\n",
        "\n",
        "The notebook illustrates first how to train a Random Forest and a using catalog parameters (Stellar Mass and Color). \n",
        "\n",
        "In the second notebook we use ANNs.\n",
        "\n",
        "We use as training set, the visually classified sample of ~14,000 galaxies by Nair&Abraham. For illustration purposes and computational time, we use jpeg RGB images as input. However the same methodology can be applied to fits.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Before we start, make sure to open this Colab notebook in \"PlayGround Mode\" (top left) and to change the Runtime type to GPU by navigating to the toolbar and clicking Runtime -> Change runtime type and then changing Hardware accelerator to GPU\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpv-XO8Sxgvi"
      },
      "source": [
        "import numpy as np\n",
        "from astropy.io import fits\n",
        "from astropy.table import Table\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, accuracy_score,auc\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PqZCKlVSMJ"
      },
      "source": [
        "## Mount Drive\n",
        "\n",
        "Before mounting the drive click on [this folder](https://drive.google.com/drive/folders/1PcftgBzBySo1Ync-Wdsp9arTCJ_MfEPE?usp=sharing) and add it to your google drive by following these steps:\n",
        "\n",
        "*   Go to your drive \n",
        "*   Find shared folder (\"Shared with me\" link)\n",
        "*   Right click it\n",
        "*   Click Add to My Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Io82GXlCNdF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqPFZXvf1FNs"
      },
      "source": [
        "---\n",
        "#### The notenook is setup to illustrate 2 different classifications:\n",
        "\n",
        "\n",
        "#### 1.   Early vs. Late: This is an easy example in which we only try to separate between early-type and late-type galaxies.\n",
        "\n",
        "#### 2.   E vs. S0: The second example is more challenging. We try to separate ellipticals from S0s.\n",
        "\n",
        "#### By default case 1 is turn on. In order to switch to case 2 set the variable CLASS_EARLY_LATE to False.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM9KbCzH21D_"
      },
      "source": [
        "CLASS_EARLY_LATE=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKshCA7yeuZs"
      },
      "source": [
        "\n",
        "\n",
        "## Data Preparation and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KcKMJO64TMq"
      },
      "source": [
        "### Load data and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqjGhUSr3uz2"
      },
      "source": [
        "We will use catalog parameters as input features (color, mass for illustration) which correlate with galaxy morphology. It is well known that early type galaxies are redder and more massive than late type galaxies. So we are going to exploit these correlations to estimate the galaxy morphology with ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dwLGPyqey_z"
      },
      "source": [
        "pathinData=\"/content/drive/My Drive/EDE2019/morphology\"\n",
        "\n",
        "if CLASS_EARLY_LATE:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S.npy') \n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML.npy') \n",
        "\n",
        "  \n",
        "\n",
        "else:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S0.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S0.npy') \n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML_E_S0.npy') \n",
        "  \n",
        "#split training and test datasets\n",
        "X_ML_train = X_ML[0:len(X_ML)//5*4,:]   \n",
        "X_ML_test = X_ML[len(X_ML)//5*4:,:]\n",
        "Y_ML_train = Y_ML[0:len(Y_ML)//5*4]\n",
        "Y_ML_test = Y_ML[len(Y_ML)//5*4:]\n",
        "I_ML_train = I_ML[0:len(I_ML)//5*4,:,:,:]\n",
        "I_ML_test = I_ML[len(Y_ML)//5*4:,:,:,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIuWoRGi4Z9Q"
      },
      "source": [
        "### Visualize some images for illustration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmzrmSJtmkji"
      },
      "source": [
        "randomized_inds_train = np.random.permutation(len(I_ML))\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in zip(randomized_inds_train[0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  im = ax.imshow(I_ML[i,:,:].astype(int))\n",
        "  plt.title('$Morph$='+str(Y_ML[i]))\n",
        "  fig.tight_layout() \n",
        "  fig.colorbar(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSvt6VH4lrv"
      },
      "source": [
        "### Visualize the feature space used for classification (Stellar Mass / Color)\n",
        "\n",
        "For the classical ML classification we are going to use 2 catalog parameters only (stellar mass and color). This means that all the information contained in the images is reduced to 2 parameters (features) which is what the algorithms see and will use for classification. The following cell plots these parameters for both classes. The two different classes are expected to have different distributions in the feature space so that the ML algorithm can partition the space. The goal is therefore to build an ML algorithm to separate the red from the blue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8cJQQxrgk0l"
      },
      "source": [
        "xlabel(\"$Log(M_*)$\", fontsize=20)\n",
        "ylabel(\"g-r\", fontsize=20)\n",
        "xlim(8,12)\n",
        "ylim(0,1.2)\n",
        "scatter(X_ML[Y_ML==1,1],X_ML[Y_ML==1,0],color='blue',s=1,label='Morph1')\n",
        "scatter(X_ML[Y_ML==0,1],X_ML[Y_ML==0,0],color='red',s=1,label='Morph0')\n",
        "legend(fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddAGx5EUvEep"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqhnEa2Z49J6"
      },
      "source": [
        "### Train RF classifier\n",
        "The first exercise you are asked is to train a Random Forest classifier. The classifer takes as input the 2 parameters (Stellar Mass and Color) and tries to predict the visual morphology. We use here a depth of 5 and default settings. You can change the parameters and explore the effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92v01tFNhM4w"
      },
      "source": [
        "# first define the classifier object called clf\n",
        "clf = RandomForestClassifier(max_depth=2)\n",
        "\n",
        "## YOU CAN CREATE SEVERAL CLASSIFIERS WITH DIFFERNET PARAMETERS SO THAT YOU CAN COMPARE THEM\n",
        "#clf_2 = \n",
        "#clf_3 = \n",
        "#...\n",
        "\n",
        "# then train the RF\n",
        "clf.fit(X_ML_train,Y_ML_train)\n",
        "\"PROVIDE HERE INPUTS FOR TRAINING - SEE DOCUMENTATION\"\n",
        "print(\"Trained RF Classifier\")\n",
        "print(clf)\n",
        "\n",
        "# The follwing allows you to see the relative importance of the different features\n",
        "print(\"Importance of each feature\")\n",
        "print(clf.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpnQJ0sVbW6F"
      },
      "source": [
        "### Visualize a random Tree\n",
        "The following tree plots a random tree from the trained RF. For an explanation of the different elements in the graph go to this [link](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76). Change the parameters of your RF classifier and explore what difference it makes on the classifcation tree below. What happens if you change the max depth from 2 to 5?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEEGN4zLacrQ"
      },
      "source": [
        "# Extract single tree - this numnber can be changed (< n_estimators)\n",
        "estimator = clf.estimators_[1]\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "# Export as dot file\n",
        "export_graphviz(estimator, out_file='tree.dot', \n",
        "                feature_names = [\"Color\",\"Mass\"],\n",
        "                class_names = [\"Early-Type\",\"Late-Type\"],\n",
        "                rounded = True, proportion = False, \n",
        "                precision = 2, filled = True)\n",
        "\n",
        "# Convert to png using system command (requires Graphviz)\n",
        "from subprocess import call\n",
        "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=50'])\n",
        "\n",
        "# Display in jupyter notebook\n",
        "from IPython.display import Image\n",
        "Image(filename = 'tree.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6jXNmKDGcyo"
      },
      "source": [
        "### Predictions and evaluation of results\n",
        "The following cells use the trained model  to predict the morphological class of the test dataset and evaluate the performance of your model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFpihOh0FuN2"
      },
      "source": [
        "print(\"Predicting...\")\n",
        "print(\"====================\")\n",
        "\n",
        "# this line is used to call the trained clf and predict in the TEST set \n",
        "Y_pred_RF=clf.predict_proba(X_ML_test)[:,1]\n",
        "#COMPLETE HERE. WHAT DATASET SHOULD YOU USE TO TEST?\"\n",
        "#global accuracy. To define the global accuracy we need to transform the sigmoid output into a binary \n",
        "# label (0/1). We use a threshold of 0.5\n",
        "\n",
        "Y_pred_RF_class=Y_pred_RF*0\n",
        "Y_pred_RF_class[Y_pred_RF>0.5]=1\n",
        "\n",
        "print(\"Global Accuracy RF:\", accuracy_score(Y_ML_test, Y_pred_RF_class))\n",
        "\n",
        "\n",
        "# ROC curve (False positive rate vs. True positive rate)\n",
        "fpr_RF, tpr_RF, thresholds_RF = roc_curve(Y_ML_test, Y_pred_RF)\n",
        "\n",
        "#plot ROC\n",
        "fig = plt.figure() \n",
        "title('ROC curve',fontsize=18)\n",
        "xlabel(\"FPR\", fontsize=20)\n",
        "ylabel(\"TPR\", fontsize=20)\n",
        "xlim(0,1)\n",
        "ylim(0,1)\n",
        "print(len(fpr_RF))\n",
        "scatter(fpr_RF,tpr_RF,c=thresholds_RF,linewidth=3,label='RF')\n",
        "plt.colorbar()\n",
        "legend(fontsize=14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eoy3MDBFUFQ3"
      },
      "source": [
        "### Bad classifcations of RFs\n",
        "The following cells visualize some random examples of bad classifications in order to explore what the classifier has understood. We also show the feature space of bad classifications to visualize. It If you run multiple times the examples will change. Run for models with different depths (from 2 to 5 for example) and comment. Can you understand the missclasifications?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro_KK9-Pvmf5"
      },
      "source": [
        "# objects classifed as early-types by the RF but visually classifed as late-types\n",
        "bad = np.where((Y_pred_RF<0.5)&(Y_ML_test==1))\n",
        "randomized_inds_train = np.random.permutation(bad)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Galaxies visually classifed as Class1 but classified as Class0\",fontsize=10)\n",
        "for i,j in zip(randomized_inds_train[0][0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  im = ax.imshow(I_ML_test[i,:,:])\n",
        "  plt.title('$Morph$='+str(Y_ML_test[i]))\n",
        "  fig.tight_layout() \n",
        "  fig.colorbar(im)\n",
        "\n",
        "\n",
        "\n",
        "# objects classifed as late-types by the CNN but visually classifed as early-types\n",
        "bad2 = ...\n",
        "## COMPLETE THIS TO SHOW EXAMPLE IMAGES OF BAD CLASSIFICATIONS USING THE EXAMPLE ABOVE\n",
        "  \n",
        "#visualize the feature space\n",
        "fig = plt.figure()\n",
        "xlabel(\"$Log(M_*)$\", fontsize=20)\n",
        "ylabel(\"g-r\", fontsize=20)\n",
        "xlim(8,12)\n",
        "ylim(0,1.2)\n",
        "scatter(X_ML_test[bad[0],1],X_ML_test[bad[0],0],color='pink',s=25,label=\"S class. as E\")\n",
        "#scatter(X_ML_test[bad2[0],1],X_ML_test[bad2[0],0],color='orange',s=25,label='E class. as S') \n",
        "legend(fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7euxVdkvTj7"
      },
      "source": [
        "## ANN Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPGGKUwbrYb1"
      },
      "source": [
        "### Train ANN classifier\n",
        "We are now going to train an Artifical Neural Network using the same input features. We use here an architecture with only one hdden layer.\n",
        "\n",
        "The following cell is used to launch TensorBoard which should enable you to visualize the training. Just run the cell. You should see an orange panel appearing. You will need Google Chrome for this to work. If it does not appear, just continue running the other cells. This will not affect the other cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8veI6OU7rok6"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir morphology/models/ann"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DXwW7H2toAw"
      },
      "source": [
        "The following cell trains the ANN. If the previous cell worked, you should see some plots appearing in the window above showing the learning history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdXBQfyJtplp"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.normalization import  BatchNormalization\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "\n",
        "pathout='morphology/models/ann' #output folder to store the results\n",
        "model_name = '/ann1'  #name of the final model which is saved in pathout\n",
        "\n",
        "\n",
        "# this is to delete ALL previous runs. Only set to True if you want to remove them.\n",
        "RESET=True\n",
        "if RESET:\n",
        "  print(\"deleting\")\n",
        "  os.system(\"rm -r \"+pathout)\n",
        "\n",
        "# some hyperparamters to be changed\n",
        "nb_epoch=50\n",
        "batch_size=50\n",
        "\n",
        "#Define callbacks to avoid more iterations once convergence\n",
        "patience_par=10\n",
        "#earlystopping = EarlyStopping(monitor='val_loss',patience = patience_par,verbose=0,mode='auto' )\n",
        "#modelcheckpoint = ModelCheckpoint(pathout+model_name+\"_best.hd5\",monitor='val_loss',verbose=0,save_best_only=True)\n",
        "tensorboard = TensorBoard(log_dir=pathout)\n",
        "\n",
        "# definition of neural network. \n",
        "# COMPLETE HERE YOUR MODEL. START SIMPLE AND ADD MORE HIDDEN LAYERS. EXPLORE THE RESUTLS\n",
        "#You can add/remove layers and see the performance. Also change the optimizers.\n",
        "ann = Sequential()\n",
        "ann.add(Flatten(input_shape=(2,1)))  ## this is your input layer. Do not change.\n",
        "########\n",
        "## ADD YOUR LAYERS HERE.\n",
        "#########\n",
        "ann.add(Dense(1,activation=\"sigmoid\"))  ## this is your ouptut layer (1 value). Do not change.\n",
        "\n",
        "# compilation of model. loss defintion. we are using binary crossentropy.\n",
        "ann.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# train the model. YOU CAN CHANGE BATCH SIZE AND NB_EPOCHS TO EXPLORE THE EFFECTS \n",
        "Xexp=np.expand_dims(X_ML_train,2)\n",
        "print(Xexp.shape)\n",
        "ann.fit(Xexp,Y_ML_train,epochs=nb_epoch,batch_size=50,callbacks=[tensorboard])\n",
        "\n",
        "# save model just in case it is needed\n",
        "ann.save(pathout+model_name+\".hd5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R566lMYvvacB"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bh6_Td_tzSP"
      },
      "source": [
        "print(\"Predicting...\")\n",
        "print(\"====================\")\n",
        "\n",
        "# set to true if you want to load a specific model. otherwise it will just use \n",
        "# the last model trained\n",
        "LOAD_MODEL=False\n",
        "if LOAD_MODEL:\n",
        "  ann = tf.keras.models.load_model(pathout+model_name+\".hd5\")\n",
        "\n",
        "\n",
        "## This line uses the trained model to predict\n",
        "Y_pred_ANN=ann.predict(np.expand_dims(X_ML_test,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwGekDd8ukqY"
      },
      "source": [
        "### Bad classifications of ANNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8B5LNsyTuPY"
      },
      "source": [
        "The follwing cells visualize some random examples of bad classifications in order to explore what the classifier has understood. We also show the feature space of bad classifications to visualize. It If you run multiple times the examples will change. Run for models with different depths (from 2 to 5 for example) and comment. Can you understand the missclasifications?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA_UqIfuuhch"
      },
      "source": [
        "# objects classifed as early-types by the ANN but visually classifed as late-types\n",
        "bad = np.where((Y_pred_ANN[:,0]<0.5)&(Y_ML_test==1))\n",
        "randomized_inds_train = np.random.permutation(bad)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Galaxies visually classifed as Class1 but classified as Class0\",fontsize=10)\n",
        "for i,j in zip(randomized_inds_train[0][0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  im = ax.imshow(I_ML_test[i,:,:])\n",
        "  plt.title('$Morph$='+str(Y_ML_test[i]))\n",
        "  fig.tight_layout() \n",
        "  fig.colorbar(im)\n",
        "\n",
        "\n",
        "\n",
        "# objects classifed as late-types by the ANN but visually classifed as early-types\n",
        "bad2 = np.where((Y_pred_ANN[:,0]>0.5)&(Y_ML_test==0))\n",
        "randomized_inds_train = np.random.permutation(bad2)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Galaxies visually classifed as Class0 but classified as Class1\",fontsize=10)\n",
        "for i,j in zip(randomized_inds_train[0][0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  im = ax.imshow(I_ML_test[i,:,:])\n",
        "  plt.title('$Morph$='+str(Y_ML_test[i]))\n",
        "  fig.tight_layout() \n",
        "  fig.colorbar(im)\n",
        "  \n",
        "#visualize the feature space\n",
        "fig = plt.figure()\n",
        "xlabel(\"$Log(M_*)$\", fontsize=20)\n",
        "ylabel(\"g-r\", fontsize=20)\n",
        "xlim(8,12)\n",
        "ylim(0,1.2)\n",
        "scatter(X_ML_test[bad[0],1],X_ML_test[bad[0],0],color='pink',s=25,label=\"S class. as E\")\n",
        "scatter(X_ML_test[bad2[0],1],X_ML_test[bad2[0],0],color='orange',s=25,label='E class. as S') \n",
        "legend(fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXKSAhBqSX2g"
      },
      "source": [
        "## Performance Estimation\n",
        "We now compute the global accuracy as well as ROC and P-R curves. If you are not familiar with these curves please see the lecture slides or click [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svvKwV-AGpYB"
      },
      "source": [
        "#global accuracy. To define the global accuracy we need to transform the sigmoid output into a binary \n",
        "# label (0/1). We use a threshold of 0.5\n",
        "\n",
        "Y_pred_RF_class=Y_pred_RF*0\n",
        "Y_pred_RF_class[Y_pred_RF>0.5]=1\n",
        "\n",
        "print(\"Global Accuracy RF:\", accuracy_score(Y_ML_test, Y_pred_RF_class))\n",
        "\n",
        "\n",
        "#global accuracy\n",
        "\n",
        "\n",
        "Y_pred_ANN_class=Y_pred_ANN*0\n",
        "Y_pred_ANN_class[Y_pred_ANN>0.5]=1\n",
        "\n",
        "\n",
        "print(\"Global Accuracy ANN:\", accuracy_score(Y_ML_test, Y_pred_ANN_class))\n",
        "\n",
        "\n",
        "\n",
        "# ROC curve (False positive rate vs. True positive rate)\n",
        "fpr_RF, tpr_RF, thresholds_RF = roc_curve(Y_ML_test, Y_pred_RF)\n",
        "fpr_ANN, tpr_ANN, thresholds_ANN = roc_curve(Y_ML_test, Y_pred_ANN)\n",
        "\n",
        "print(\"AUC RF:\", auc(fpr_RF, tpr_RF))\n",
        "\n",
        "#plot ROC\n",
        "fig = plt.figure() \n",
        "title('ROC curve',fontsize=18)\n",
        "xlabel(\"FPR\", fontsize=20)\n",
        "ylabel(\"TPR\", fontsize=20)\n",
        "xlim(0,1)\n",
        "ylim(0,1)\n",
        "scatter(fpr_RF,tpr_RF,linewidth=3,c=thresholds_RF,label='RF')\n",
        "scatter(fpr_ANN,tpr_ANN,linewidth=3,c=thresholds_ANN,label='ANN')\n",
        "legend(fontsize=14)\n",
        "plt.colorbar()\n",
        "\n",
        "\n",
        "\n",
        "# Precision Recall curve (False positive rate vs. True positive rate)\n",
        "\n",
        "## PLOT HERE THE PRECISION-RECALL CURVE AS DONE FOR THE ROC CURVE. COMMENT THE DIFFERENCES.\n",
        "\n",
        "## COMPUTE THESE 3 DIAGNOSTICS FOR DIFFERENT MODELS AND COMPARE THEM IN THE SAME PLOTS. COMMENT."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}