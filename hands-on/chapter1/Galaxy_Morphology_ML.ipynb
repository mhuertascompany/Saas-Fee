{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/Saas-Fee/blob/main/hands-on/chapter1/Galaxy_Morphology_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jatIhJ9KJQV"
      },
      "source": [
        "#Galaxy Morphology with \"classical ML\"\n",
        "\n",
        "The goal of these tutorial series is to illustrate a very basic supervised binary classification with different ML approaches. The goal is to setup a ML algorithm to determine the visual morphological type of nearby galaxies from the Sloan Digital Sky Survey. The first deep learning papers in Astronomy addressed this problem at low and high redshift (Dielemann+15, Huertas-Company+15).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1TaiRB1wxui4AKnhuF4iH4LJkmrlb-D6d)\n",
        "\n",
        "The notebook illustrates first how to train several Machine Learning Classifiers using catalog parameters (Stellar Mass and Color to start with).\n",
        "\n",
        " We use as training set, the visually classified sample of ~14,000 galaxies by Nair&Abraham.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpv-XO8Sxgvi"
      },
      "source": [
        "import numpy as np\n",
        "from astropy.io import fits\n",
        "from astropy.table import Table\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "import pdb\n",
        "import pickle\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, accuracy_score,auc\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PqZCKlVSMJ"
      },
      "source": [
        "## Mount Drive\n",
        "\n",
        "Before mounting the drive click on [this folder](https://drive.google.com/drive/folders/1PcftgBzBySo1Ync-Wdsp9arTCJ_MfEPE?usp=sharing) and add it to your google drive by following these steps:\n",
        "\n",
        "*   Go to your drive\n",
        "*   Find shared folder (\"Shared with me\" link)\n",
        "*   Right click it\n",
        "*   \"Add Shortcut to Drive\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Io82GXlCNdF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqPFZXvf1FNs"
      },
      "source": [
        "---\n",
        "#### The notenook is setup to illustrate 2 different classifications:\n",
        "\n",
        "\n",
        "#### 1.   Early vs. Late: This is an easy example in which we only try to separate between early-type and late-type galaxies.\n",
        "\n",
        "#### 2.   E vs. S0: The second example is more challenging. We try to separate ellipticals from S0s.\n",
        "\n",
        "#### By default case 1 is turn on. In order to switch to case 2 set the variable CLASS_EARLY_LATE to False.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM9KbCzH21D_"
      },
      "source": [
        "CLASS_EARLY_LATE=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKshCA7yeuZs"
      },
      "source": [
        "## Ex. 1: Random Forest Classifer Elliptical/Spiral with 2 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KcKMJO64TMq"
      },
      "source": [
        "### Load data and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqjGhUSr3uz2"
      },
      "source": [
        "For the classical approaches, the input are catalog parameters (color, mass for illustration) which correlate with galaxy morphology. It is well known that early type galaxies are redder and more massive than late type galaxies. So we are going to exploit these correlation to estimate the galaxy morphology."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dwLGPyqey_z"
      },
      "source": [
        "pathinData=\"/content/drive/My Drive/ED127_2023/morphology\"\n",
        "\n",
        "if CLASS_EARLY_LATE:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S.npy')\n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML.npy')\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S0.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S0.npy')\n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML_E_S0.npy')\n",
        "\n",
        "#split training and test datasets\n",
        "X_ML_train = X_ML[0:len(X_ML)//5*4,:]\n",
        "X_ML_test = X_ML[len(X_ML)//5*4:,:]\n",
        "Y_ML_train = Y_ML[0:len(Y_ML)//5*4]\n",
        "Y_ML_test = Y_ML[len(Y_ML)//5*4:]\n",
        "I_ML_train = I_ML[0:len(I_ML)//5*4,:,:,:]\n",
        "I_ML_test = I_ML[len(Y_ML)//5*4:,:,:,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIuWoRGi4Z9Q"
      },
      "source": [
        "### Visualize some images for illustration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmzrmSJtmkji"
      },
      "source": [
        "randomized_inds_train = np.random.permutation(len(I_ML))\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in zip(randomized_inds_train[0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  im = ax.imshow(I_ML[i,:,:].astype(int))\n",
        "  plt.title('$Morph$='+str(Y_ML[i]))\n",
        "  fig.tight_layout()\n",
        "  fig.colorbar(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSvt6VH4lrv"
      },
      "source": [
        "### Visualize the feature space used for classification (Stellar Mass / Color)\n",
        "\n",
        "For the classical ML classification we are going to use 2 catalog parameters only (stellar mass and color). This means that all the information contained in the images is reduced to 2 parameters (features) which is what the algorithms see and will use for classification. The following cell plots these parameters for both classes. The two different classes are expected to have different distributions in the feature space so that the ML algorithm can partition the space. The goal is therefore to build an ML algorithm to separate the red from the blue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8cJQQxrgk0l"
      },
      "source": [
        "xlabel(\"$Log(M_*)$\", fontsize=20)\n",
        "ylabel(\"g-r\", fontsize=20)\n",
        "xlim(8,12)\n",
        "ylim(0,1.2)\n",
        "scatter(X_ML[Y_ML==1,1],X_ML[Y_ML==1,0],color='blue',s=1,label='Morph1')\n",
        "scatter(X_ML[Y_ML==0,1],X_ML[Y_ML==0,0],color='red',s=1,label='Morph0')\n",
        "legend(fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqhnEa2Z49J6"
      },
      "source": [
        "### Train RF classifier\n",
        "The first exercise you are asked is to train a Random Forest classifier. The classifer takes as input the 2 parameters (Stellar Mass and Color) and tries to predict the visual morphology. You can change the hyper parameters and explore the effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92v01tFNhM4w"
      },
      "source": [
        "# you can add the classifiers into a list to access them later\n",
        "classifiers = []\n",
        "\n",
        "\n",
        "for md, nest in zip ([2,10,100],[10,100,500]):\n",
        "\n",
        "# first define the classifier object called clf\n",
        "  clf = RandomForestClassifier(max_depth=md, n_estimators=nest)\n",
        "\n",
        "## YOU CAN CREATE SEVERAL CLASSIFIERS WITH DIFFERNET PARAMETERS SO THAT YOU CAN COMPARE THEM.\n",
        "## TRY CHANGING THE MAX_DEPTH (e.g. 2,10,100) AND N_ESTIMATORS PARAMETER (e.g. 10,100,500)\n",
        "#clf_2 = RandomForestClassifier()\n",
        "#clf_3 =  RandomForestClassifier()\n",
        "#...\n",
        "\n",
        "# then train the RF\n",
        "  clf.fit(X_ML_train,Y_ML_train)\n",
        "  classifiers.append(clf)\n",
        "# ADD OTHER CLASSIFIERS\n",
        "\n",
        "\n",
        "  # The follwing allows you to see the relative importance of the different features\n",
        "  print(\"Importance of each feature\")\n",
        "  print(clf.feature_importances_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpnQJ0sVbW6F"
      },
      "source": [
        "### Visualize a random Tree\n",
        "The following tree plots a random tree from the trained RF. For an explanation of the different elements in the graph go to this [link](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76). Change the parameters of your RF classifier and explore what difference it makes on the classifcation tree below. What happens if you change the max depth from 2 to 5?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEEGN4zLacrQ"
      },
      "source": [
        "\n",
        "class_number = 0\n",
        "\n",
        "# Extract single tree - this numnber can be changed (< n_estimators)\n",
        "estimator = classifiers[class_number].estimators_[1]\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "# Export as dot file\n",
        "export_graphviz(estimator, out_file='tree.dot',\n",
        "                feature_names = [\"Color\",\"Mass\"],\n",
        "                class_names = [\"Early-Type\",\"Late-Type\"],\n",
        "                rounded = True, proportion = False,\n",
        "                precision = 2, filled = True)\n",
        "\n",
        "# Convert to png using system command (requires Graphviz)\n",
        "from subprocess import call\n",
        "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=50'])\n",
        "\n",
        "# Display in jupyter notebook\n",
        "from IPython.display import Image\n",
        "Image(filename = 'tree.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6jXNmKDGcyo"
      },
      "source": [
        "### Predictions and evaluation of results\n",
        "The following cells use the trained model  to predict the morphological class of the test dataset and evaluate the performance of your model. It is assumed that the different RF classifiers are in a list. Howwever, feel free to change the implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFpihOh0FuN2"
      },
      "source": [
        "\n",
        "Y_pred_RF=[[0] * len(X_ML_test) for i in range(len(classifiers))]\n",
        "\n",
        "for i,rf in enumerate(classifiers):\n",
        "\n",
        "  print(\"Predicting...\")\n",
        "  print(\"====================\")\n",
        "\n",
        "  # this line is used to call the trained clf and predict in the TEST set\n",
        "\n",
        "  Y_pred_RF[i]=rf.predict_proba(\"COMPLETE\")[:,1] # predict_proba returns one column/class. We take only one.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHx5qNzXh8vF"
      },
      "source": [
        "### Visualization of decision boundaries\n",
        "Plot the decision boundaries of the different RF classifiers. You can use the function introduced in the previous session or code a new one. Comment the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTYS14FkCcpS"
      },
      "source": [
        "# this piece of code creates a mesh grid to cover the parameter space\n",
        "h = .02  # step size in the mesh\n",
        "x_min, x_max = X_ML_train[:, 0].min() - 1, X_ML_train[:, 0].max() + 1\n",
        "y_min, y_max = X_ML_train[:, 1].min() - 1, X_ML_train[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# title for the plots\n",
        "titles = ['RF1',\n",
        "          'RF2',\n",
        "          'RF3'\n",
        "          ]\n",
        "\n",
        "for i, clf in enumerate(classifiers):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max] x[y_min, y_max].\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # call the classifer for each point in the grid\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(yy, xx, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X_ML_train[:, 1], X_ML_train[:, 0], c=Y_ML_train, cmap=plt.cm.coolwarm,s=1)\n",
        "    plt.xlabel('stellar mass')\n",
        "    plt.ylabel('color')\n",
        "    plt.ylim(xx.min(), xx.max())\n",
        "    plt.xlim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXKSAhBqSX2g"
      },
      "source": [
        "We now compute the global accuracy as well as ROC and P-R curves. If you are not familiar with these curves please see the lecture slides or click [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svvKwV-AGpYB"
      },
      "source": [
        "#global accuracy. To define the global accuracy we need to transform the sigmoid output into a binary\n",
        "# label (0/1). We use a threshold of 0.5\n",
        "\n",
        "color=['blue','red','green']\n",
        "\n",
        "#plot ROC\n",
        "fig = plt.figure()\n",
        "title('ROC curve',fontsize=18)\n",
        "xlabel(\"FPR\", fontsize=20)\n",
        "ylabel(\"TPR\", fontsize=20)\n",
        "xlim(0,1)\n",
        "ylim(0,1)\n",
        "\n",
        "for i in range(len(classifiers)):\n",
        "  Y_pred_RF_class=Y_pred_RF[i]*0\n",
        "  Y_pred_RF_class[np.array(Y_pred_RF[i])>0.5]=1\n",
        "  print(\"Global Accuracy RF:\"+str(i), accuracy_score(Y_ML_test, Y_pred_RF_class))\n",
        "  # ROC curve (False positive rate vs. True positive rate)\n",
        "  ## PLOT ROC curve\n",
        "  legend(fontsize=14)\n",
        "\n",
        "\n",
        "# Precision Recall curve\n",
        "\n",
        "## REPEAT STEPS WITH P-R CURVE. (Look at this function for more details: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8B5LNsyTuPY"
      },
      "source": [
        "The follwing cells visualize some random examples of bad classifications in order to explore what the classifier has understood. We also show the feature space of bad classifications to visualize. It If you run multiple times the examples will change. Run for models with different depths (from 2 to 5 for example) and comment. Can you understand the missclasifications?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eoy3MDBFUFQ3"
      },
      "source": [
        "### Bad classifcations of RFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro_KK9-Pvmf5"
      },
      "source": [
        "class_number = 0 # choose the classifier\n",
        "\n",
        "# objects classifed as early-types by the RF but visually classifed as late-types\n",
        "bad = np.where((Y_pred_RF[class_number]<0.5)&(Y_ML_test==1))\n",
        "randomized_inds_train = np.random.permutation(bad)\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Galaxies visually classifed as Class1 but classified as Class0\",fontsize=10)\n",
        "for i,j in zip(randomized_inds_train[0][0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  im = ax.imshow(I_ML_test[i,:,:])\n",
        "  plt.title('$Morph$='+str(Y_ML_test[i]))\n",
        "  fig.tight_layout()\n",
        "  fig.colorbar(im)\n",
        "\n",
        "\n",
        "\n",
        "# objects classifed as late-types by the RF but visually classifed as early-types\n",
        "# COMPLETE\n",
        "##bad2 = ...\n",
        "\n",
        "#visualize the feature space\n",
        "fig = plt.figure()\n",
        "xlabel(\"$Log(M_*)$\", fontsize=20)\n",
        "ylabel(\"g-r\", fontsize=20)\n",
        "xlim(8,12)\n",
        "ylim(0,1.2)\n",
        "scatter(X_ML_test[bad[0],1],X_ML_test[bad[0],0],color='pink',s=25,label=\"S class. as E\")\n",
        "scatter(X_ML_test[bad2[0],1],X_ML_test[bad2[0],0],color='orange',s=25,label='E class. as S')\n",
        "legend(fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9JUuI38bHkc"
      },
      "source": [
        "## Ex. 2 Can you repeat the steps above with an SVM classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuFpod9nbj86"
      },
      "source": [
        "Explore the documentation of scikitlearn and try to implement an SVM based classifier for the binary problem above. Explore bad classifications and plot in the same plot the ROC curves of the RF and SVM classifiers. Comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSPbNDhduADd"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# TRY 3 DIFFERENT SVM CLASSIFIERS WITH 3 TYPES OF KERNELS (LINEAR, RBF AND POLYNOMIAL)\n",
        "svm_classifiers =[]\n",
        "C = 1.0  # SVM regularization parameter\n",
        "svc = svm_classifiers.append(svm.SVC(kernel='linear', C=C,probability=True).fit(X_ML_train, Y_ML_train))\n",
        "#rbf_svc = ...\n",
        "#poly_svc = ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGTVzfmgB1x7"
      },
      "source": [
        "Plot the decision boundaries of the different SVM classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amoMo6qwujJg"
      },
      "source": [
        "## ADD CODE TO PLOT DECISION BOUNDARIES OF SVM CLASSIFIERS\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X_ML_train[:, 1], X_ML_train[:, 0], c=Y_ML_train, cmap=plt.cm.coolwarm,s=1)\n",
        "    plt.xlabel('stellar mass')\n",
        "    plt.ylabel('color')\n",
        "    plt.ylim(xx.min(), xx.max())\n",
        "    plt.xlim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_g3k7uQCNLw"
      },
      "source": [
        "Predict and plot ROC and P-R curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGKTjBUT6nAT"
      },
      "source": [
        "Y_pred_SVM=[[0] * len(X_ML_test) for i in range(len(svm_classifiers))]\n",
        "\n",
        "for rf,i in zip(svm_classifiers,range(len(svm_classifiers))):\n",
        "\n",
        "  print(\"Predicting...\")\n",
        "  print(\"====================\")\n",
        "\n",
        "  # this line is used to call the trained clf and predict in the TEST set\n",
        "  Y_pred_SVM[i]=rf.predict_proba(X_ML_test)[:,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrPOaL436alh"
      },
      "source": [
        "## ADD CODE TO PLOT ROC AND P-R CURVES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFg3hiQvYd4M"
      },
      "source": [
        "## Ex 3: Random Forest Classifier E/S0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_jrYrH0lh89"
      },
      "source": [
        "Build a RF or an SVM classifier for the E/S0 classification. Plot in the same figure the ROC and P-R curves for the two cases. Explore the bad classifications. Comment the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmeqrrSEmZ7u"
      },
      "source": [
        "CLASS_EARLY_LATE=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QW8o_ED80V7"
      },
      "source": [
        "pathinData=\"/content/drive/My Drive/ED127_2022/morphology\"\n",
        "\n",
        "if CLASS_EARLY_LATE:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S.npy')\n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML.npy')\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S0.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S0.npy')\n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML_E_S0.npy')\n",
        "\n",
        "#split training and test datasets\n",
        "X_ML_train = X_ML[0:len(X_ML)//5*4,:]\n",
        "X_ML_test = X_ML[len(X_ML)//5*4:,:]\n",
        "Y_ML_train = Y_ML[0:len(Y_ML)//5*4]\n",
        "Y_ML_test = Y_ML[len(Y_ML)//5*4:]\n",
        "I_ML_train = I_ML[0:len(I_ML)//5*4,:,:,:]\n",
        "I_ML_test = I_ML[len(Y_ML)//5*4:,:,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi7LWJqH87QF"
      },
      "source": [
        "randomized_inds_train = np.random.permutation(len(I_ML))\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in zip(randomized_inds_train[0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  im = ax.imshow(I_ML[i,:,:].astype(int))\n",
        "  plt.title('$Morph$='+str(Y_ML[i]))\n",
        "  fig.tight_layout()\n",
        "  fig.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go6ac6KY8_JD"
      },
      "source": [
        "xlabel(\"$Log(M_*)$\", fontsize=20)\n",
        "ylabel(\"g-r\", fontsize=20)\n",
        "xlim(8,12)\n",
        "ylim(0,1.2)\n",
        "scatter(X_ML[Y_ML==1,1],X_ML[Y_ML==1,0],color='blue',s=1,label='Morph1')\n",
        "scatter(X_ML[Y_ML==0,1],X_ML[Y_ML==0,0],color='red',s=1,label='Morph0')\n",
        "legend(fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDLgxoOJ9CeN"
      },
      "source": [
        "classifiers_ES0 = []\n",
        "\n",
        "# first define the classifier object called clf\n",
        "#clf = ...\n",
        "\n",
        "## YOU CAN CREATE SEVERAL CLASSIFIERS WITH DIFFERNET PARAMETERS SO THAT YOU CAN COMPARE THEM\n",
        "#clf_2 = ...\n",
        "#clf_3 =  ...\n",
        "#...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R_-xkiwmuIu"
      },
      "source": [
        "## Ex. 4: Increasing the number of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvJb4vqBmznx"
      },
      "source": [
        "In the previous sections we have used only 2 parameters to perform the classification. We will try now to use more dimensions. The following cell loads a catalog with 5 parameters: stellar mass, color, Sersic Index, Velocity Dispersion and axis ratio. Repeat the steps above for the E/S0 case with different sets of parameters, compare and comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fJkuCZZnF2S"
      },
      "source": [
        "pathinData=\"/content/drive/My Drive/ED127_2022/morphology\"\n",
        "\n",
        "if CLASS_EARLY_LATE:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S_large.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S_large.npy')\n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML_large.npy')\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "  # donwload feature vector and labels\n",
        "  X_ML = np.load(pathinData+'/feature_E_S0_large.npy')\n",
        "  #morphological class\n",
        "  Y_ML = np.load(pathinData+'/label_E_S0_large.npy')\n",
        "  #we also load images (for visualization purposes - not used for training)\n",
        "  I_ML=np.load(pathinData+'/images_ML_E_S0_large.npy')\n",
        "\n",
        "#split training and test datasets\n",
        "X_ML_train = X_ML[0:len(X_ML)//5*4,:]\n",
        "X_ML_test = X_ML[len(X_ML)//5*4:,:]\n",
        "Y_ML_train = Y_ML[0:len(Y_ML)//5*4]\n",
        "Y_ML_test = Y_ML[len(Y_ML)//5*4:]\n",
        "I_ML_train = I_ML[0:len(I_ML)//5*4,:,:,:]\n",
        "I_ML_test = I_ML[len(Y_ML)//5*4:,:,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFB3fjlFBufD"
      },
      "source": [
        "classifiers_ES0 = []\n",
        "\n",
        "# first define the classifier object called clf\n",
        "#clf = ...\n",
        "\n",
        "## YOU CAN CREATE SEVERAL CLASSIFIERS WITH DIFFERNET PARAMETERS SO THAT YOU CAN COMPARE THEM\n",
        "#clf_2 =\n",
        "#clf_3 =\n",
        "#...\n",
        "\n",
        "# then train the RF\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Hqv2PV6Vp5"
      },
      "source": [
        "## Ex 4. Boosting algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4U-3BO_DxQK"
      },
      "source": [
        "Can you improve the results using [Boosting Algorithms](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) ? You can take it any of the previuos ML algorithms (RF or SVM) and apply a boosting algorithm such as AdaBoost. Plot ROC and P-R curves and compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2YUQ7kL6TNp"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "abc = AdaBoostClassifier(\"complete\")\n",
        "\n",
        "#Training\n",
        "abc.fit(\"complete\")\n",
        "Y_pred_boost = abc.predict_proba(\"complete\")[:,1]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-445WHzpC4yG"
      },
      "source": [
        "Plot ROC and P-R"
      ]
    }
  ]
}