{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_SDSS_spectra.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/Saas-Fee/blob/main/hands-on/session3/VAE_SDSS_spectra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNma6PFg51c9"
      },
      "source": [
        "# VAE FOR GALAXY SDSS SPECTRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwR6rBQI5Xeb"
      },
      "source": [
        "The goal of this tutorial is to see if VAEs are able to caputre the different galaxy types (e.g. star-forming, passive, AGNs..) from the unsupervised decomposition of spectra. See [Portillo+18](https://ui.adsabs.harvard.edu/abs/2020arXiv200210464P/abstract)\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Before we start, make sure to open this Colab notebook in \"PlayGround Mode\" (top left) and to change the Runtime type to GPU by navigating to the toolbar and clicking Runtime -> Change runtime type and then changing Hardware accelerator to GPU\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fGHe0YVSO5"
      },
      "source": [
        "## Import packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH8UWWdr4h0M"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "import pickle\n",
        "from astropy.io import fits\n",
        "from astropy.visualization.stretch import SqrtStretch\n",
        "from astropy.visualization import ImageNormalize, MinMaxInterval\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PqZCKlVSMJ"
      },
      "source": [
        "## Data download and preparation\n",
        "\n",
        "Before mounting the drive click on [this folder](https://drive.google.com/drive/folders/1PcftgBzBySo1Ync-Wdsp9arTCJ_MfEPE?usp=sharing) and add it to your google drive by following these steps:\n",
        "\n",
        "*   Go to your drive \n",
        "*   Find shared folder (\"Shared with me\" link)\n",
        "*   Right click it\n",
        "*   Click Add to My Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKksKNJmgeQQ"
      },
      "source": [
        "Mount your drive into Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Io82GXlCNdF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGiXJnno9fFI"
      },
      "source": [
        "Then load the data for training. The dataset (X) contains 2 numpy arrays with the galaxy images (128*128 pixels) and the known effective radii for every galaxy. For the training we are using Single Sersic Models convolved with an HST PSF and with real HST noise added. The effective size of the modeled galaxies is thus known and stored in vector Y. The goal is to estimate Y from X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhIqCwEg9UUd"
      },
      "source": [
        "#Load data\n",
        "pathinData=\"/content/drive/My Drive/EDE2019/spectra\"\n",
        "\n",
        "#images\n",
        "spectra = np.load(pathinData+'/flux.npy')\n",
        "\n",
        "#wavelength\n",
        "\n",
        "wl = np.load(pathinData+'/wl.npy')\n",
        "\n",
        "#labels\n",
        "label = np.load(pathinData+'/bpt_labels.npy') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUc4GM_PeRG0"
      },
      "source": [
        "## Visualize Spectra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yAnZuFuXMpt"
      },
      "source": [
        "stretch = SqrtStretch() \n",
        "\n",
        "randomized_inds_train = np.random.permutation(len(spectra))\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in zip(randomized_inds_train[0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  plt.plot(wl,np.log10(100*spectra[i,:]))\n",
        "  #im = ax.imshow(x_train[i,:,:,0], origin='lower', cmap='gray',\n",
        "    #vmin=vmin, norm=norm,vmax=vmax)\n",
        "  plt.title('BPT='+str(label[i]))\n",
        "  fig.tight_layout() \n",
        "  #fig.colorbar(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwYTplEJy-i"
      },
      "source": [
        "sp=spectra[0,np.where((wl>4000)&(wl<7000))]\n",
        "print(len(sp[0]))\n",
        "\n",
        "sp2=spectra[:,np.where((wl>4000)&(wl<7000))]\n",
        "print(sp2.shape)\n",
        "sp2=np.reshape(sp2,(sp2.shape[0],sp2.shape[2],1))\n",
        "print(sp2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh5-oVoOWEMv"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXsoNEpR6mYH"
      },
      "source": [
        "Your goal is to setup a VAE that takes as input the spectra we plotted above and learns to generate them. Then you can plot the embeded space and see if things do cluster. Feel free to try differnt approaches! And compare with PCA..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF_8j309IRCn"
      },
      "source": [
        "#NEURAL NETWORK - CONVOLUTIONAL AUTOENCODER\n",
        "sp=spectra[0,:]\n",
        "original_dim = sp2.shape[1]\n",
        "print(original_dim)\n",
        "\n",
        "\n",
        "input_shape = (original_dim,1)\n",
        "encoded_size = 5  # THIS IS THE SIZE OF THE BOTTLENECK --> FEEL FREE TO CHANGE\n",
        "# AND EXPLORE\n",
        "base_depth = 32\n",
        "\n",
        "\n",
        "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
        "                        reinterpreted_batch_ndims=1)\n",
        "\n",
        "encoder = tfk.Sequential([\n",
        "    tfkl.InputLayer(input_shape=input_shape),\n",
        "    #tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n",
        "    tfkl.Conv1D(base_depth, 10, strides=1,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.Conv1D(base_depth, 10, strides=4,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.Conv1D(2 * base_depth, 5, strides=1,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.Conv1D(2 * base_depth, 5, strides=4,\n",
        "                padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.Conv1D(4 * encoded_size, 4, strides=4,\n",
        "                padding='valid', activation=tf.nn.leaky_relu),\n",
        "    tfkl.Flatten(),\n",
        "    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
        "               activation=None),\n",
        "    tfpl.MultivariateNormalTriL(\n",
        "        encoded_size,\n",
        "        activity_regularizer=tfpl.KLDivergenceRegularizer(prior,weight=0.01)),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "decoder = tfk.Sequential([\n",
        "    tfkl.InputLayer(input_shape=[encoded_size]),\n",
        "    tfkl.Reshape([1, encoded_size]),\n",
        "    tfkl.Conv1D(2 * base_depth, 3, strides=1,\n",
        "                         padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.UpSampling1D(size=4),                      \n",
        "    tfkl.Conv1D(2 * base_depth, 3, strides=1,\n",
        "                         padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.UpSampling1D(size=4),                      \n",
        "    tfkl.Conv1D(2 * base_depth, 5, strides=1,\n",
        "                         padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.UpSampling1D(size=2),                      \n",
        "    tfkl.Conv1D(base_depth, 5, strides=1,\n",
        "                         padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.UpSampling1D(size=2),\n",
        "    tfkl.Conv1D(base_depth, 10, strides=1,\n",
        "                         padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.UpSampling1D(size=2),                      \n",
        "    tfkl.Conv1D(base_depth, 10, strides=1,\n",
        "                         padding='same', activation=tf.nn.leaky_relu),\n",
        "    tfkl.UpSampling1D(size=2),                      \n",
        "    tfkl.Conv1D(filters=1, kernel_size=5, strides=1,\n",
        "                padding='same', activation=tf.nn.tanh),\n",
        "    tfkl.Flatten(),\n",
        "    \n",
        "    tfkl.Dense(tfpl.IndependentNormal.params_size(input_shape),activation=None),\n",
        "    #tfpl.IndependentNormal(input_shape, tfd.Normal.mean),\n",
        "    tfpl.IndependentNormal(input_shape, tfd.Normal.sample),\n",
        "    #tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits),\n",
        "])\n",
        "\n",
        "\n",
        "vae = tfk.Model(inputs=encoder.inputs,outputs=decoder(encoder.outputs[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmLXxPzLeF8v"
      },
      "source": [
        "encoder.summary()\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdxa34j-7woD"
      },
      "source": [
        "negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
        "\n",
        "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
        "            loss=negloglik)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "hist = vae.fit(np.arcsinh(sp2),np.arcsinh(sp2),epochs=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SijKheTSCayK"
      },
      "source": [
        "#predict 10 examples\n",
        "\n",
        "xhat = vae(np.arcsinh(sp2[10:20]))\n",
        "assert isinstance(xhat, tfd.Distribution)\n",
        "\n",
        "print(spectra.shape)\n",
        "#pdb.set_trace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8ImerAH8kL8"
      },
      "source": [
        "## Plotting the results\n",
        "Plot here the results of your embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXZkt7zhC-uS"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i,j in zip(range(10,20),range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  plt.plot(wl[np.where((wl>4000)&(wl<7000))],np.arcsinh(sp2[i,:]))\n",
        "  #plt.title('$Class$='+str(y_test[i]))\n",
        "  fig.tight_layout() \n",
        "  #fig.colorbar(im)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in zip(range(10),range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  plt.plot(wl[np.where((wl>4000)&(wl<7000))],xhat.sample()[i,:])\n",
        "  #plt.title('$Class$='+str(y_test[i]))\n",
        "  fig.tight_layout() \n",
        "  #fig.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWG9osSSLhUM"
      },
      "source": [
        "xhat = encoder(np.arcsinh(sp2))\n",
        "assert isinstance(xhat, tfd.Distribution)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV6F7JAuLw1d"
      },
      "source": [
        "z=np.asarray(xhat.sample())\n",
        "print(z.shape)\n",
        "\n",
        "dim1=0\n",
        "dim2=4\n",
        "\n",
        "plt.scatter(z[np.where(label==-1),dim1],z[np.where(label==-1),dim2],c='red',s=2)\n",
        "plt.scatter(z[np.where(label==5),dim1],z[np.where(label==5),dim2],c='blue',s=2)\n",
        "plt.scatter(z[np.where(label==1),dim1],z[np.where(label==1),dim2],c='green',s=2)\n",
        "plt.scatter(z[np.where(label==2),dim1],z[np.where(label==2),dim2],c='orange',s=2)\n",
        "plt.scatter(z[np.where(label==3),dim1],z[np.where(label==3),dim2],c='pink',s=2)\n",
        "plt.scatter(z[np.where(label==4),dim1],z[np.where(label==4),dim2],c='black',s=2)\n",
        "plt.scatter(z[np.where(label==6),dim1],z[np.where(label==6),dim2],c='yellow',s=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coyU9W48Oek0"
      },
      "source": [
        "import umap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "reducer = umap.UMAP()\n",
        "scaled_z = StandardScaler().fit_transform(z)\n",
        "embedding = reducer.fit_transform(scaled_z)\n",
        "embedding.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B0Vh-hMOzBD"
      },
      "source": [
        "\n",
        "import seaborn as sns\n",
        "plt.scatter(\n",
        "    embedding[:, 0],\n",
        "    embedding[:, 1],\n",
        "    c=label,s=5)\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}