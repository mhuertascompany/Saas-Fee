{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sersic_Reff_tfp_Saas-Fee.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/Saas-Fee/blob/main/hands-on/session3/Sersic_Reff_tfp_Saas_Fee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNma6PFg51c9"
      },
      "source": [
        "# GALAXY SIZES WITH CNNs AND TENSORFLOW PROBABILITY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwR6rBQI5Xeb"
      },
      "source": [
        "The purpose of this tutorial is to estimate galaxy half light radii of galaxies from images. Galaxy sizes are traditionnally estimated by fitting a [PSF-convolved Sersic model](https://users.obs.carnegiescience.edu/peng/work/galfit/galfit.html) to galaxy images. We are going to use a CNN instead. The network takes then as input a galaxy image convolved with a PSF and outputs its effective radius. The same approach can be used to estiamte other parameters such as ellipticiites. See [Tuccillo+18](http://adsabs.harvard.edu/abs/2018MNRAS.475..894T) for an illustration. In this tutorial we are going a step further by estimating the posterior distribution of sizes instead of a single point estimate. The posterior is modeled with a gaussian mixture model using TF probability.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1R9ITrzxUG0X9VjwFUhbbwsDhZD8A4ANy)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Before we start, make sure to open this Colab notebook in \"PlayGround Mode\" (top left) and to change the Runtime type to GPU by navigating to the toolbar and clicking Runtime -> Change runtime type and then changing Hardware accelerator to GPU\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fGHe0YVSO5"
      },
      "source": [
        "## Import packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH8UWWdr4h0M"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "import pickle\n",
        "from astropy.io import fits\n",
        "from astropy.visualization.stretch import SqrtStretch\n",
        "from astropy.visualization import ImageNormalize, MinMaxInterval\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PqZCKlVSMJ"
      },
      "source": [
        "## Data download and preparation\n",
        "\n",
        "Before mounting the drive click on [this folder](https://drive.google.com/drive/folders/1PcftgBzBySo1Ync-Wdsp9arTCJ_MfEPE?usp=sharing) and add it to your google drive by following these steps:\n",
        "\n",
        "*   Go to your drive \n",
        "*   Find shared folder (\"Shared with me\" link)\n",
        "*   Right click it\n",
        "*   Click Add to My Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKksKNJmgeQQ"
      },
      "source": [
        "Mount your drive into Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Io82GXlCNdF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGiXJnno9fFI"
      },
      "source": [
        "Then load the data for training. The dataset (X) contains 2 numpy arrays with the galaxy images (128*128 pixels) and the known effective radii for every galaxy. For the training we are using Single Sersic Models convolved with an HST PSF and with real HST noise added. The effective size of the modeled galaxies is thus known and stored in vector Y. The goal is to estimate Y from X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhIqCwEg9UUd"
      },
      "source": [
        "#Load data\n",
        "pathinData=\"/content/drive/My Drive/EDE2019/sizes\"\n",
        "\n",
        "#images\n",
        "X = np.load(pathinData+'/Stamps_Simulated_Galaxies_tutorial.npy')\n",
        "\n",
        "#sizes\n",
        "Y = np.load(pathinData+'/Parameters_Simulated_Galaxies_tutorial.npy') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5KbWjmdIp8Y"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler,  MinMaxScaler\n",
        "# rescaling the sizes with mean and variance\n",
        "for i in range(len(X)):\n",
        "  X[i,:,:]=X[i,:,:]/np.max(X[i,:,:])\n",
        "scalerY =  MinMaxScaler().fit(Y.reshape((-1,1)))\n",
        "Ys = scalerY.transform(Y.reshape((-1,1)))\n",
        "Y_o=Y\n",
        "#scaler = StandardScaler().fit(X)\n",
        "#Xs = scaler.transform(Xs)\n",
        "\n",
        "\n",
        "# Spliting in Training and Test datasets, 4/5 of galaxies for training, 1/5 for testing\n",
        "x_train = X[0:len(X)//5*4,0,:,:]   \n",
        "x_test = X[len(X)//5*4:,0,:,:]\n",
        "t_train = Ys[0:len(Ys)//5*4,:]\n",
        "t_test = Ys[len(Ys)//5*4:,:]\n",
        "        \n",
        "print(np.max(t_train),np.min(t_train))\n",
        "\n",
        "x_train=np.expand_dims(x_train,axis=3)\n",
        "x_test=np.expand_dims(x_test,axis=3)\n",
        "print ('Y_train.shape= ', t_train.shape)  \n",
        "print ('X_train.shape= ', x_train.shape) \n",
        "print ('Y_test.shape= ', t_test.shape)  \n",
        "print ('X_test.shape= ', x_test.shape) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUc4GM_PeRG0"
      },
      "source": [
        "## Visualize images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yAnZuFuXMpt"
      },
      "source": [
        "stretch = SqrtStretch() \n",
        "\n",
        "randomized_inds_train = np.random.permutation(len(x_train))\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in zip(randomized_inds_train[0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  interval = MinMaxInterval()\n",
        "  vmin, vmax = interval.get_limits(x_train[i,:,:])\n",
        "  norm = ImageNormalize(stretch=stretch)\n",
        "  im = ax.imshow(x_train[i,:,:,0], origin='lower', cmap='gray',\n",
        "    vmin=vmin, norm=norm,vmax=vmax)\n",
        "  plt.title('$R_e$='+str(Y_o[i]))\n",
        "  fig.tight_layout() \n",
        "  fig.colorbar(im)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh5-oVoOWEMv"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXsoNEpR6mYH"
      },
      "source": [
        "The follwing cells define the neural network architecture. We use here the Tensorflow Esitmator API. The function _mdn_model_fn defines the model. It is made of 6 convolutional layers with some pooling operations. The output of the convolutional part is then fed into a fully connected NN which estimates the scales, means and standard deviations of the gaussian PDFs of the mixture model. The proposed version is found to be quite stable. However, feel free to modify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHNytp8PRhr"
      },
      "source": [
        "negloglik = lambda y, p_y: -p_y.log_prob(y)  ## THIS DEFINES THE LOSS - CAN YOU MAKE SENSE OF IT?\n",
        "\n",
        "\n",
        "#Define output path - This folder will contain the trained model\n",
        "pathout='models/model1'\n",
        "\n",
        "#define callbacks\n",
        "tensorboard = TensorBoard(log_dir=pathout)\n",
        "\n",
        "def build_model(nfilters,num_components):\n",
        "  cnn = tfk.Sequential([\n",
        "  tfkl.Conv2D(\n",
        "      nfilters, (4,4),\n",
        "      input_shape=(128,128,1),\n",
        "      padding=\"same\",\n",
        "      activation='relu'),\n",
        "  tfkl.BatchNormalization(),\n",
        "  ## CONVOLUTIONAL LAYERS TO BE ADDED HERE   \n",
        "  \n",
        "  tf.keras.layers.Flatten(),      \n",
        "  ## DENSE LAYERS HERE\n",
        "  tfkl.Dense(64, activation='tanh'),\n",
        "  tfkl.Dense(units=num_components*3,activation=None), \n",
        "\n",
        "  ## THE OUTPUT IS A MIXTURE OF NORMAL PDFs WITH NUM_COMPONENTS     \n",
        "  tfpl.DistributionLambda(lambda t: tfd.Independent(\n",
        "        tfd.MixtureSameFamily(mixture_distribution=tfd.Categorical(logits=tf.expand_dims(t[..., :num_components], -2)),\n",
        "                              components_distribution=tfd.Normal(tf.nn.softplus(tf.expand_dims(t[..., num_components:2*num_components], -2)),\n",
        "                                                               tf.nn.softplus(tf.expand_dims(t[..., 2*num_components:],-2)))), 1))\n",
        "    ])     \n",
        "\n",
        "\n",
        "  cnn.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00002),loss=negloglik)\n",
        "\n",
        "  return cnn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyfbQGD67bkO"
      },
      "source": [
        "## Training and Predicting\n",
        "The code starts here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg3NBkZoiZjK"
      },
      "source": [
        "\n",
        "\n",
        "#Set RESET=True to delete all previous runs of the same model\n",
        "RESET=False\n",
        "if RESET:\n",
        "  tf.summary.FileWriterCache.clear()\n",
        "  !rm -rf pathout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3ruvXs3W1QD"
      },
      "source": [
        "We first train the model for 1000 steps. This is only a trick to be able to use TensorBoard within the notebook. The current implementation uses 2 mixture models. This can be modified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GsM4egTYCeT"
      },
      "source": [
        "Then launch tensorboard. An orange panel with all TensorBoard panels should appear. If not try running the cell again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdgmpCedIJ6Q"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir models/model1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpNIWqx9Yahh"
      },
      "source": [
        "Continue training. The TensorBoard panel should update automatically so that you can track the progress. The maximum number of iterations is set to 10,000 to speed up computation. However the model continues improving at leat up to 200,000 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0rtel_Fb3Q3"
      },
      "source": [
        "nb_epochs=20\n",
        "batch_size=32\n",
        "\n",
        "cnn = build_model(16,1)\n",
        "\n",
        "print(\"Training...\")\n",
        "history = cnn.fit(x_train,t_train,batch_size=batch_size,epochs=nb_epochs,callbacks=[tensorboard])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YBm1wDCM5M3"
      },
      "source": [
        "plot(history.history['loss'],linewidth=2)\n",
        "plt.xlabel('Epochs',fontsize=20)\n",
        "plt.ylabel('Loss',fontsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tip1LLws24Sc"
      },
      "source": [
        "We now look at the posterior distributions. The first cell creates the posterior distirbutions by predicting on the test dataset. The second cell plots the posteriors along with the true values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr6aoCYd92JL"
      },
      "source": [
        "\n",
        "outputs = cnn(x_test[0:50])\n",
        "size = np.linspace(-50, 50,100)\n",
        "sizet = scalerY.transform(size.reshape((-1,1)))\n",
        "logps = []\n",
        "\n",
        "for i in range(len(sizet)):\n",
        "    logps.append(outputs.log_prob(sizet[i]).numpy())\n",
        "logps = np.stack(logps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aBy2gBUChlE"
      },
      "source": [
        "y_test = Y_o[len(Ys)//5*4:]\n",
        "for i in range(10):\n",
        "  figure()\n",
        "  plot(size, np.exp(logps[:,-i]), label='posterior under training prior')\n",
        "  axvline(y_test[-i], color='m', label='True value')\n",
        "  xlabel(r'$R_e$')\n",
        "  legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASvwSAz5YwtF"
      },
      "source": [
        "We now use the saved model to estimate the sizes of galaxies in the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8ImerAH8kL8"
      },
      "source": [
        "## Plotting the results\n",
        "The follwing cells do some plots of the results. Comment. Can you guess why the scatter increases at large sizes? \n",
        "Exercice: Repeat the above steps but using now an image size of $64\\times64$ instead of the default $128\\times128$. You should only modify the reading cell and the CNN input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1ABEr1xOcfF"
      },
      "source": [
        "m = scalerY.inverse_transform(cnn(x_test[0:2000]).mean().numpy().reshape(-1,1))\n",
        "s = scalerY.inverse_transform(cnn(x_test[0:2000]).stddev().numpy().reshape(-1,1))\n",
        "fig = plt.figure() \n",
        "scatter(y_test[0:2000], m,c=s ,alpha=0.7)\n",
        "plt.colorbar()\n",
        "xlabel('Input Re',fontsize=20)\n",
        "ylabel('Predicted Re',fontsize=20)\n",
        "plot([0,50],[0,50],'k--')\n",
        "xlim(0,35)\n",
        "ylim(0,35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CSHDM2Lc69U"
      },
      "source": [
        "## Real data\n",
        "The training dataset was made of simple analytic simulations of galaxies. We now try to apply our model to real observations from the Hubble Space Telescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXB19f_fc_A8"
      },
      "source": [
        "#reads imgaes\n",
        "X_real = np.load(pathinData+'/RealStamps_tutorial.npy')\n",
        "for i in range(len(X_real)):\n",
        "  X_real[i,:,:]=X_real[i,:,:]/np.max(X_real[i,:,:])\n",
        "\n",
        "#size vector\n",
        "Y_real = np.load(pathinData+'/ParametersRealStamps_tutorial.npy')\n",
        "\n",
        "\n",
        "X_real = X_real[:,0,:,:] \n",
        "\n",
        "Y_real=Y_real.reshape(-1,1)\n",
        "Y_real_o=np.copy(Y_real)\n",
        "Y_real= scalerY.transform(Y_real.reshape((-1,1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovevUZB5huFz"
      },
      "source": [
        "print(Y_real.shape)\n",
        "print(X_real.shape)\n",
        "\n",
        "stretch = SqrtStretch() \n",
        "\n",
        "randomized_inds_test = np.random.permutation(len(X_real))\n",
        "print(np.min(x_train))\n",
        "print(np.min(X_real))\n",
        "\n",
        "fig = plt.figure()\n",
        "for i,j in zip(randomized_inds_test[0:4],range(4)):\n",
        "  ax = fig.add_subplot(2, 2, j+1)\n",
        "  interval = MinMaxInterval()\n",
        "  vmin, vmax = interval.get_limits(X_real[i,:,:])\n",
        "  norm = ImageNormalize(stretch=stretch)\n",
        "  im = ax.imshow(X_real[i,:,:], origin='lower', cmap='gray',\n",
        "    vmin=vmin, norm=norm,vmax=vmax)\n",
        "  plt.title('$R_e$='+str(Y_real_o[i]))\n",
        "  fig.tight_layout() \n",
        "  fig.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEF_2tVO0hQj"
      },
      "source": [
        "##PLOT HERE THE POSTERIOR DISTRIBUTIONS OF THE REAL DATASET AS DONE FOR THE SIMULATED DATASET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QocT-sQi4Xoz"
      },
      "source": [
        "## PLOT HERE A SCATTER PLOT OF GROUND TRUTH VERSUS PREDICTED AS DONE FOR THE SIMULATED DATASET"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}