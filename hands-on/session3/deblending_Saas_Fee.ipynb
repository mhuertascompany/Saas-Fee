{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deblending_Saas-Fee.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/Saas-Fee/blob/main/hands-on/session3/deblending_Saas_Fee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TllvwK7ogni"
      },
      "source": [
        "# Galaxy deblending with Deep Convolutional Encoder-Decoders\n",
        "\n",
        "##### Alexandre Boucaud & Marc Huertas-Company "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYUZkGjQ00td"
      },
      "source": [
        "In astronomical images, especially in deep surveys such as EUCLID, the projection effects may cause two or more galaxies to overlap. When they are barely indistinguishable from one another, they are referred to as _blended_ and this can bias astrophysical estimators such as the morphology of galaxies or the shear.  \n",
        "\n",
        "As the sensitivity of imaging devices grows, a high fraction of galaxies appear _blended_ in the images (LSST estimate around ~50%-70%), which is a known and important issue for current and upcoming galaxy surveys.  \n",
        "\n",
        "There are many approaches to deblending (separating ovelapping galaxies). The goal of this notebook is to illustrate how a particular deep network architecture called Convotuional Encoder-Decoder can approach the problem.\n",
        "\n",
        "For the purpose of this example, we use a simplfied dataset as in [Boucaud+19](http://adsabs.harvard.edu/abs/2019arXiv190501324B). We use a set of observed isolated galaxies with HST that we artificially overlap. We therefore know the positions of the original galaxies before adding them and we can use this information for training.\n",
        "\n",
        "We keep always one galaxy in the center of the stamp and the second one can be in any position within the stamp.  All blended stamps contain only 2 galaxies. The goal of this tutorial is to set up an Encoder-Decoder to recover the pixels belonging to the companion galaxy. See example below.\n",
        "\n",
        "The goal here is to isolate from the image of overalpping galaxies, the pixels which belong ONLY to the companion galaxy.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1m5v14u4aKVtn3QBMXx4GcZqMjt5gRWSO)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Before we start, make sure to open this Colab notebook in \"PlayGround Mode\" (top left) and to change the Runtime type to GPU by navigating to the toolbar and clicking Runtime -> Change runtime type and then changing Hardware accelerator to GPU\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-U9GfSaVX9o"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PqZCKlVSMJ"
      },
      "source": [
        "## Data download and preparation\n",
        "\n",
        "Before mounting the drive click on [this folder](https://drive.google.com/drive/folders/1PcftgBzBySo1Ync-Wdsp9arTCJ_MfEPE?usp=sharing) and add it to your google drive by following these steps:\n",
        "\n",
        "*   Go to your drive \n",
        "*   Find shared folder (\"Shared with me\" link)\n",
        "*   Right click it\n",
        "*   Click Add to My Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Io82GXlCNdF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3UL4LY330-k"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ8C-hsXVrW8"
      },
      "source": [
        "suffix = \"\" \n",
        "pathinData=\"/content/drive/My Drive/EDE2019/deblending\"\n",
        "\n",
        "X_train = np.load(os.path.join(pathinData, f\"train_blends{suffix}.npy\"), mmap_mode='r')\n",
        "Y_train = np.load(os.path.join(pathinData, f\"train_target_masks{suffix}.npy\"), mmap_mode='r')[:, 1, :, :]\n",
        "\n",
        "X_test = np.load(os.path.join(pathinData, f\"test_blends{suffix}.npy\"), mmap_mode='r')\n",
        "Y_test = np.load(os.path.join(pathinData, f\"test_target_masks{suffix}.npy\"), mmap_mode='r')[:, 1, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpyzBpli33Xm"
      },
      "source": [
        "### Plot a random example\n",
        "Run multiple times to see more than one example. The left panel shows the image with 2 overlapping galaxies. The right panel shows the target segmentation mask we want to achieve. Only the pixels belonging to the companion are set to 1, the rest of the image is set to 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olHmjWzTXKuf"
      },
      "source": [
        "def plot_data_basic(idx):\n",
        "    titles = [\n",
        "        'blended galaxies',\n",
        "        'segmap of companion galaxy'\n",
        "    ]\n",
        "\n",
        "    fig_size = (12, 6)\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=fig_size)\n",
        "    axes[0].imshow(X_train[idx], cmap='viridis')\n",
        "    axes[1].imshow(Y_train[idx], cmap='Greys_r')\n",
        "    for title, ax in zip(titles, axes):\n",
        "        ax.set_title(title)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "index = np.random.randint(len(X_train))\n",
        "plot_data_basic(index)\n",
        "print((Y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSg9lTx76yuh"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "The following cells set up the model in TensorFlow. We have set up a very basic Convolutional Encoder-Decoder. It does not perform super well for this problem. I encourage you to try to modify this network to achieve better results. You can have a look at Boucaud+19 for inspiration. Otherwise, just Google \"Image Segmentation with Deep Learning\". There are many solutions. A very popular approach is called the U-net. Can you code it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl8xTMBwXxSY"
      },
      "source": [
        "def pack_images(images, rows, cols):\n",
        "    \"\"\"Helper utility to make a field of images.\"\"\"\n",
        "    shape = tf.shape(images)\n",
        "    width = shape[-3]\n",
        "    height = shape[-2]\n",
        "    depth = shape[-1]\n",
        "    images = tf.reshape(images, (-1, width, height, depth))\n",
        "    batch = tf.shape(images)[0]\n",
        "    rows = tf.minimum(rows, batch)\n",
        "    cols = tf.minimum(batch // rows, cols)\n",
        "    images = images[:rows * cols]\n",
        "    images = tf.reshape(images, (rows, cols, width, height, depth))\n",
        "    images = tf.transpose(images, [0, 2, 1, 3, 4])\n",
        "    images = tf.reshape(images, [1, rows * width, cols * height, depth])\n",
        "    return images\n",
        "\n",
        "\n",
        "def image_tile_summary(name, tensor, rows=8, cols=8):\n",
        "    tf.summary.image(name, pack_images(tensor, rows, cols), max_outputs=1)\n",
        "\n",
        "def fcnn_model():\n",
        " \n",
        "    \n",
        "    input_layer = tf.keras.Input(shape=(128,128,1))\n",
        " \n",
        "    # Convolutional Layer #1\n",
        "    conv1 = tf.keras.layers.Conv2D(\n",
        "      32,\n",
        "      (3, 3),\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)(input_layer)\n",
        "    \n",
        "    #pooling\n",
        "    pool1 = tf.keras.layers.MaxPooling2D((2,2))(conv1)\n",
        "    \n",
        "    #Convolutional Layer #2\n",
        "    conv2 = tf.keras.layers.Conv2D(\n",
        "      64,\n",
        "      (3, 3),\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)(pool1)\n",
        "    \n",
        "    #pooling  \n",
        "    pool2=tf.keras.layers.MaxPooling2D((2,2))(conv2)\n",
        "    \n",
        "    \n",
        "\n",
        "    # Deconvolutional Layer #1\n",
        "    deconv1 = tf.keras.layers.Conv2DTranspose(\n",
        "      64,\n",
        "      (3,3), \n",
        "      padding=\"same\",\n",
        "      strides=(2,2),\n",
        "      activation=tf.nn.relu)(pool2)\n",
        "    \n",
        "    # Deconvolutional Layer #2\n",
        "    deconv2 = tf.keras.layers.Conv2DTranspose(\n",
        "      32,\n",
        "      (3,3), \n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)(deconv1)\n",
        "    \n",
        "    # Deconvolutional Layer #3 (NEED TO MAKE SURE THAT THE LAST LAYER IS OF SIZE 128*128*!)\n",
        "    output = tf.keras.layers.Conv2DTranspose(\n",
        "      1,\n",
        "      (3,3), \n",
        "      padding=\"same\",\n",
        "      strides=(2,2),\n",
        "      activation='sigmoid')(deconv2)\n",
        "    \n",
        "    #predictions = tf.nn.sigmoid(output)\n",
        "    \n",
        "\n",
        "\n",
        "    return tf.keras.Model(input_layer,output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-kt84SKLwMe"
      },
      "source": [
        "## INCLUDE THE CODE OF YOUR MODEL HERE\n",
        "\n",
        "## CAN YOU CREATE A FCNN MODEL THAT GETS THE BEST IOU SCORE?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MltdwyRD8O_c"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg3NBkZoiZjK"
      },
      "source": [
        "#Define output path - This folder will contain the trained model\n",
        "pathout='deblending/models/model1'\n",
        "\n",
        "#Set RESET=True to delete all previous runs of the same model\n",
        "RESET=False\n",
        "if RESET:\n",
        "  os.system(\"rm -rf \"+ pathout)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86kAD7QxL45u"
      },
      "source": [
        "Run TensorBoard. You should see an orange panel appearing. If it's not the case, try again running the cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdgmpCedIJ6Q"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir deblending/models/fcnn1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKCy2m9vMA3Q"
      },
      "source": [
        "Main training cell. You should be able to follow the training progress in the TensorBoard window above. If you click on the tab IMAGES you will see examples of output images during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0rtel_Fb3Q3"
      },
      "source": [
        "nb_epochs=50\n",
        "batch_size=16\n",
        "\n",
        "#Define output path - This folder will contain the trained model\n",
        "pathout='deblending/models/fcnn1'\n",
        "\n",
        "#define callbacks\n",
        "tensorboard = TensorBoard(log_dir=pathout)\n",
        "\n",
        "fcnn = fcnn_model()\n",
        "fcnn.compile(loss='binary_crossentropy',optimizer=tf.optimizers.Adam(learning_rate=0.0001))\n",
        "\n",
        "print(\"Training...\")\n",
        "history=fcnn.fit(X_train,Y_train,batch_size=batch_size,epochs=nb_epochs,callbacks=[tensorboard])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCyEH_j3ogoD"
      },
      "source": [
        "## Evaluation of the model \n",
        "\n",
        "For image detection a classical metric is the ***Intersection over Union (IoU)*** also referred to as ***Jaccard index*** and defined as\n",
        "\n",
        "$$ IoU(A, B) =  \\dfrac{|A \\cap B|}{|A \\cup B|} $$\n",
        "\n",
        "This metric is very sensitive to small shifts or area difference between truth and prediction.\n",
        "\n",
        "Typically, a value of IoU superior to 0.5 is used to define a good detection.\n",
        "\n",
        "An implementation of the IoU for a series of flatten segmentation images $\\in [0, 1]$ can be found below. You can just compile it.\n",
        "\n",
        "The goal is to perform a model that gets the best IoU score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CcK2PTV8qyK"
      },
      "source": [
        "def iou_bitmap(y_true, y_pred, verbose=False):\n",
        "    \"\"\"\n",
        "    Compute the IoU between two arrays\n",
        "    If the arrays are probabilities (floats) instead of predictions (integers\n",
        "    or booleans) they are automatically rounded to the nearest integer and\n",
        "    converted to bool before the IoU is computed.\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : ndarray\n",
        "        array of true labels\n",
        "    y_pred : ndarray\n",
        "        array of predicted labels\n",
        "    verbose : bool (optional)\n",
        "        print the intersection and union separately\n",
        "    Returns\n",
        "    -------\n",
        "    float :\n",
        "        the intersection over union (IoU) value scaled between 0.0 and 1.0\n",
        "    \"\"\"\n",
        "    EPS = np.finfo(float).eps\n",
        "\n",
        "    # Make sure each pixel was predicted e.g. turn probability into prediction\n",
        "    if y_true.dtype in [np.float32, np.float64]:\n",
        "        y_true = y_true.round().astype(bool)\n",
        "\n",
        "    if y_pred.dtype in [np.float32, np.float64]:\n",
        "        y_pred = y_pred.round().astype(bool)\n",
        "\n",
        "    # Reshape to 1d\n",
        "    y_true = y_true.ravel()\n",
        "    y_pred = y_pred.ravel()\n",
        "\n",
        "    # Compute intersection and union\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    sum_ = np.sum(y_true + y_pred)\n",
        "    jac = (intersection + EPS) / (sum_ - intersection + EPS)\n",
        "\n",
        "    if verbose:\n",
        "        print('Intersection:', intersection)\n",
        "        print('Union:', sum_ - intersection)\n",
        "\n",
        "    return jac\n",
        "\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    iou_list = [iou_bitmap(yt, yp)\n",
        "                for (yt, yp) in zip(y_true, y_pred)]\n",
        "    return np.mean(iou_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktPrcjg8_R0W"
      },
      "source": [
        "Compute IoU for the current model. If you are happy with your model, go here: https://keepthescore.co/board/txlibfdenre/ and enter your IoU score (in %)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVJU4VJZ88hb"
      },
      "source": [
        "Y_p = fcnn(X_test[0:50])\n",
        "\n",
        "#p_mask=[]\n",
        "#mask=[]\n",
        "#for i in range(len(Y_test[0:50])):\n",
        "#    p_mask.append(next(Y_p))\n",
        "#    mask.append(Y_test[i])\n",
        "\n",
        "  \n",
        "s = iou(np.asarray(Y_p[0:50]).squeeze(),Y_test[0:50].squeeze())\n",
        "print('IoU score:', s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkebY1hqGmcs"
      },
      "source": [
        "fig_size = (10, 12)\n",
        "n_gal=5\n",
        "fig, ax = plt.subplots(nrows=n_gal, ncols=4, figsize=fig_size)\n",
        "titles = [\n",
        "            'blend',\n",
        "            'true segmentation',\n",
        "            'output',\n",
        "            'output thresholded',\n",
        "        ]\n",
        "\n",
        "for i in range(n_gal):\n",
        "  img = np.squeeze(X_test[i])\n",
        "  yt = np.squeeze(Y_test[i])\n",
        "  yp = np.squeeze(np.asarray(Y_p)[i])\n",
        "  ax[i, 0].imshow(img)\n",
        "  ax[i, 1].imshow(yt)\n",
        "  ax[i, 2].imshow(yp)\n",
        "  ax[i, 3].imshow(yp.round())\n",
        "  if i == 0:\n",
        "    for idx, a in enumerate(ax[i]):\n",
        "      a.set_title(titles[idx])\n",
        "  for a in ax[i]:\n",
        "    a.set_axis_off()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amTlZq6S_Vn1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}