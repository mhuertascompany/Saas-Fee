{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuertascompany/Saas-Fee/blob/main/hands-on/chapter2/hello_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHrlbIjlVE7G"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "from matplotlib import patches\n",
        "from matplotlib import lines\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from astropy.wcs import WCS\n",
        "from astropy.coordinates import SkyCoord\n",
        "from astropy.nddata.utils import Cutout2D\n",
        "import scipy.stats as stats\n",
        "import sys\n",
        "from scipy.ndimage import uniform_filter\n",
        "from astropy.table import Table\n",
        "from astropy.cosmology import Planck13\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PsvDVEjVE7N"
      },
      "source": [
        "# Let's first generate some data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-cxKTr8VE7N"
      },
      "source": [
        "x = np.random.uniform(-1,1,100)\n",
        "y = 0.1*x+np.random.normal(0,0.025,100)\n",
        "plt.scatter(x,y,label='data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lxez5J_VE7O"
      },
      "source": [
        "# The normal way to deal with this, is through linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrBPV302VE7O"
      },
      "source": [
        "res = np.polyfit(x,y,1)\n",
        "print(res)\n",
        "plt.scatter(x,y,label='data')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiSadI8eVE7O"
      },
      "source": [
        "# Now, let's try to write the linear regression in a different way (more complicated way)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "YleKEGp-VE7P"
      },
      "source": [
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions\n",
        "\n",
        "ann = tfk.Sequential([\n",
        "tf.keras.layers.Flatten(input_shape=(1,1)),\n",
        "tfkl.Dense(1, activation=None)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESychuhYVE7P"
      },
      "source": [
        "The Dense command here, onnly says that the input is multiplied by a parameter $w$. We are effectively writing a simple model for our data: $y = w.a+b$, where $w$ is unknown.\n",
        "![alt](https://drive.google.com/uc?id=1Rt2bNPCxaHXdjzmVS7TCw_u_Ur-WIqlW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmG2cEfWVE7P"
      },
      "source": [
        "We can visualize the model we just created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzc-PSxWVE7P"
      },
      "source": [
        "ann.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts2m-GUXVE7Q"
      },
      "source": [
        " # We then compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hhtREiKnVE7Q"
      },
      "source": [
        "ann.compile(optimizer=tf.optimizers.Adam(),loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ax3CgZFVE7Q"
      },
      "source": [
        "We are simply tht we want to minimize the mean square error (mse) between input and output. We call this the \"loss function\". So we are looking for the value of $w$ that minimizes the following expression: $$ \\sum(x-w.x)^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc9SHME0VE7Q"
      },
      "source": [
        "# And fit the model ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDGy4f97VE7R"
      },
      "source": [
        "# you might need to run this cell a couple of times if it does not work directly\n",
        "ann.fit(x,y,batch_size=1,epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JImVcZLyVE7R"
      },
      "source": [
        "# Let's see what we got here..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEtm7-q_VE7R"
      },
      "source": [
        "xp = np.linspace(-1,1)\n",
        "yp = ann.predict(xp)\n",
        "plt.plot(xp,yp,color='red',label='ANN')\n",
        "plt.scatter(x,y,label='data')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IncN226-VE7R"
      },
      "source": [
        "We have performed a linear regression with and artifical neural network ! So, yes, linear regression IS also Machine Learning..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zpwhMgfVE7S"
      },
      "source": [
        "# But why is this useful ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6y3vMj7VE7S"
      },
      "source": [
        "Let's suppose we have a more complex dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3CVeXr2ZVE7S"
      },
      "source": [
        "x = np.random.uniform(-1,1,100)\n",
        "y = 0.1*x+np.sin(5*x)+np.random.normal(0,0.2,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKurFICVE7S"
      },
      "source": [
        "plt.scatter(x,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWeuuDrAVE7S"
      },
      "source": [
        "# I can try again simple linear regression ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVkCaIdqVE7S"
      },
      "source": [
        "res = np.polyfit(x,y,1)\n",
        "print(res)\n",
        "plt.scatter(x,y,label='data')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chYup1A3VE7T"
      },
      "source": [
        "but that will not work super well as expected..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIFjpJT1VE7T"
      },
      "source": [
        "# Let's go back to our complicated ANN ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKxmTy8SVE7T"
      },
      "source": [
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "ann = tfk.Sequential([\n",
        "tf.keras.layers.Flatten(input_shape=(1,1)),\n",
        "tfkl.Dense(1, activation=None)])\n",
        "ann.compile(optimizer=tf.optimizers.Adam(),loss='mse')\n",
        "ann.fit(x,y,batch_size=1,epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-bDbTHVE7T"
      },
      "source": [
        "xp = np.linspace(-1,1)\n",
        "yp = ann.predict(xp)\n",
        "plt.plot(xp,yp,color='red',label='ANN')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.scatter(x,y)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwCzDHC6VE7U"
      },
      "source": [
        "If I do not change anything, I will obtain the same result. My model is simply linear..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLosTF5eVE7U"
      },
      "source": [
        "# Let's add a bit of non-linearity ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XXo_txJiVE7U"
      },
      "source": [
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "ann = tfk.Sequential([\n",
        "tf.keras.layers.Flatten(input_shape=(1,1)),\n",
        "tfkl.Dense(1, activation='sigmoid')])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLD4zYR7VE7V"
      },
      "source": [
        "The sigmoid function is given by this expression: $$ \\frac{1}{1+e^{-x}}$$\n",
        "So our model is now like this:![alt](https://drive.google.com/uc?id=1-2VbatzRnqGJMKCga-tppiTo6iPRBr9s)\n",
        "This is what we call a perceptron. The non-linear function added after the linear combination is also called the activation function, because \"it fires the unit\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9hbqfFhVE7V"
      },
      "source": [
        "ann.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYHMWcnnVE7V"
      },
      "source": [
        "ann.compile(optimizer=tf.optimizers.Adam(),loss='mse')\n",
        "ann.fit(x,y,batch_size=1,epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cabQ6_IkVE7W"
      },
      "source": [
        "xp = np.linspace(-1,1)\n",
        "yp = ann.predict(xp)\n",
        "plt.plot(xp,yp,color='red',label='ANN')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.scatter(x,y)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK_EjsP9VE7W"
      },
      "source": [
        "Still not great, but there is some potential !?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSWG7_EvVE7X"
      },
      "source": [
        "# We are going to work a bit more on the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gcm0tb5SVE7X"
      },
      "source": [
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "ann = tfk.Sequential([\n",
        "tf.keras.layers.Flatten(input_shape=(1,1)),\n",
        "tfkl.Dense(1, activation='sigmoid'),\n",
        "tfkl.Dense(1, activation=None)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYiXwT_rVE7X"
      },
      "source": [
        "We have added \"a layer\". Our model is now: $$ y=(\\frac{1}{1+e^{-(w_1.x)}}).w_2$$\n",
        "![alt](https://drive.google.com/uc?id=1E0iobni7jhUI2jfGKPb081OM_QDB5Hjg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD3h4rTuVE7X"
      },
      "source": [
        "ann.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meaCUYihVE7Y"
      },
      "source": [
        "ann.compile(optimizer=tf.optimizers.Adam(),loss='mse')\n",
        "ann.fit(x,y,batch_size=1,epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKXYFTacVE7Y"
      },
      "source": [
        "xp = np.linspace(-1,1)\n",
        "yp = ann.predict(xp)\n",
        "plt.plot(xp,yp,color='red',label='ANN')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.scatter(x,y)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUEkYCI5VE7Y"
      },
      "source": [
        "Not fantastic, but you get the idea...You have just created your first ANN for regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priNDi2BVE7Y"
      },
      "source": [
        "In fact, it turns out that it exists a mathematical theorem that proves that NNs are optimal approximators:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACHz-ClLVE7Y"
      },
      "source": [
        "FOR ANY CONTINUOS FUNCTION FOR A HYPERCUBE [0,1]d TO REAL NUMBERS, AND EVERY POSITIVE EPSILON, THERE EXISTS A SIGMOID BASED 1-HIDDEN LAYER NEURAL NETWORK THAT OBTAINES AT MOST EPSILON ERROR IN FUNCTIONAL SPACE - Cybenko+89"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL3KMJM2VE7Z"
      },
      "source": [
        "“BIG ENOUGH NETWORK CAN APPROXIMATE, BUT NOT REPRESENT ANY SMOOTH FUNCTION. THE MATH DEMONSTRATION IMPLIES SHOWING THAT NETWORS ARE DENSE IN THE SPACE OF TARGET FUNCTIONS”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykn8zyUfVE7Z"
      },
      "source": [
        "So, the approximation theorem tells me that there exists a NN that can approximate any function. It does not tell me which one: this is the alchemia of ML. It does not tell me how to minimize it either!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHkTxckiVE7Z"
      },
      "source": [
        "# Let's then try to improve it...anyway."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "SFlcILTEVE7Z"
      },
      "source": [
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "ann = tfk.Sequential([\n",
        "tf.keras.layers.Flatten(input_shape=(1,1)),\n",
        "tfkl.Dense(30, activation='relu'),\n",
        "tfkl.Dense(20, activation='relu'),\n",
        "tfkl.Dense(10, activation='relu'),\n",
        "tfkl.Dense(5, activation='relu'),\n",
        "tfkl.Dense(1, activation=None)])\n",
        "ann.compile(optimizer=tf.optimizers.Adam(),loss='mse')\n",
        "ann.fit(x,y,batch_size=1,epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpLMLdiPVE7a"
      },
      "source": [
        "xp = np.linspace(-1,1)\n",
        "yp = ann.predict(xp)\n",
        "plt.plot(xp,yp,color='red',label='ANN')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.scatter(x,y,label='data')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s766SrdWVE7a"
      },
      "source": [
        "Which is not that far from the real underlying model..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U22nQ7AVE7a"
      },
      "source": [
        "xp = np.linspace(-1,1)\n",
        "yp = ann.predict(xp)\n",
        "plt.plot(xp,yp,color='red',label='ANN')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.scatter(x,y,label='data')\n",
        "plt.plot(np.linspace(-1,1),0.1*np.linspace(-1,1)+np.sin(5*np.linspace(-1,1)),label='model',color='black')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TRDpzyIVE7a"
      },
      "source": [
        "Does it mean I can do any arbitrarily complex network and I still will be able to optimize over it? Yes! The answer is the backpropagation algorithm (Rumelhart et al., 1986a)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHO__tRbVE7a"
      },
      "source": [
        "So this is it? If I add a lot of different layers, am I doing Deep Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuaqZc5pVE7a"
      },
      "source": [
        "Well, not yet ... deep learning implies also feautre learning, which we have not touched here. However, the framework is the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o9gDwfOVE7a"
      },
      "source": [
        "# What about errors? Can we capture the uncertainties in the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rK051ndzVE7b"
      },
      "source": [
        "\n",
        "ann = tfk.Sequential([\n",
        "tf.keras.layers.Flatten(input_shape=(1,1)),\n",
        "tfkl.Dense(30, activation='relu'),\n",
        "tfkl.Dense(20, activation='relu'),\n",
        "tfkl.Dense(10, activation='relu'),\n",
        "tfkl.Dense(5, activation='relu'),\n",
        "tfkl.Dense(tfpl.IndependentNormal.params_size(1),activation=None),\n",
        "tfpl.IndependentNormal(1, tfd.Normal.sample)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxc2_S5JVE7b"
      },
      "source": [
        "Wow! What's that? We are transforming our model into a probabilsiitc model. Our model now predicts a Normal pdf at every point. We are going to learn the mean and the stanrdarde deviation of the pdf. That way, we let the model capture not only the mean but also the uncertainity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_5yIhLnVE7b"
      },
      "source": [
        "So now, let's compile this model. Since the output of the network is now a distribution, we are going to maximize the likelihood, or minimize the negative log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc8rhW5DVE7b"
      },
      "source": [
        "negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
        "ann.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001),loss=negloglik)\n",
        "ann.fit(x,y,batch_size=1,epochs=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSHyRxuMVE7c"
      },
      "source": [
        "# Let's plot the results ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySfxLvIoVE7c"
      },
      "source": [
        "xp = np.linspace(-1,1)\n",
        "yp = ann(xp).mean()\n",
        "yp_std = ann(xp).stddev()\n",
        "plt.plot(xp,yp,color='red',label='ANN (mean)')\n",
        "plt.plot(xp,yp+yp_std,color='red',label='ANN (std)',ls='--')\n",
        "plt.plot(xp,yp-yp_std,color='red',label='ANN (std)',ls='--')\n",
        "plt.plot(np.linspace(-1,1),np.linspace(-1,1)*res[0]+res[1],color='green',label='polyfit')\n",
        "plt.scatter(x,y,label='data')\n",
        "plt.plot(np.linspace(-1,1),0.1*np.linspace(-1,1)+np.sin(5*np.linspace(-1,1)),label='model',color='black')\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL4_l8scVE7c"
      },
      "source": [
        "The model captures now that it is more uncertain towrds the edges of the distribution..."
      ]
    }
  ]
}